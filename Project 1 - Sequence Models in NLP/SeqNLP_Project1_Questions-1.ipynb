{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SeqNLP_Project1_Questions-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n",
        "\n",
        "\n",
        "### Generate Word Embeddings and retrieve outputs of each layer with Keras based on Classification task\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
        "\n",
        "It is a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
        "\n",
        "We willl use the imdb dataset to learn word embeddings as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with sentiment (positive or negative). \n",
        "\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "`from keras.datasets import imdb`\n",
        "\n",
        "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000.\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "\n",
        "### Aim\n",
        "\n",
        "1. Import test and train data  \n",
        "2. Import the labels ( train and test) \n",
        "3. Get the word index and then Create key value pair for word and word_id. (12.5 points)\n",
        "4. Build a Sequential Model using Keras for Sentiment Classification task. (10 points)\n",
        "5. Report the Accuracy of the model. (5 points)  \n",
        "6. Retrive the output of each layer in keras for a given single test sample from the trained model you built. (2.5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "#### Usage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "706a4ec2-4ea6-469c-d2ba-7b5a3ec39ae3"
      },
      "source": [
        "from keras.datasets import imdb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3nn5S4OXK-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000 #vocab size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcTAKhBsXn8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# save np.load\n",
        "np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77GuJ0L_XghA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnma7pxkX01X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.load = np_load_old"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQJelX0KWqbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 300  #number of word used from each review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-5UZVBhWqb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEY4W5l7Wqb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras import callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt-4DZqFWqb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3acd66ec-0ea2-467e-d714-28f7cbf908c0"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU9jdTKIYQmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c427b6f9-c348-49b9-87fa-276347517dc5"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI7W5c8dYXM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8712f232-83e0-4d30-8ed6-ff64281bed7b"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aTv5JNYYaiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fd6f052-a5fd-48a5-ffb3-02ce99e03ee4"
      },
      "source": [
        "y_test.shape\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9cL2a7MYepD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0feb2b51-b9b3-4360-ca18-a1e75e298aff"
      },
      "source": [
        "print(np.unique(y_train))\n",
        "print(np.unique(y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdJncXagWqb5",
        "colab_type": "text"
      },
      "source": [
        "## Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wwCIXHbWqb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "edf1a740-8cf9-4c01-9f1a-5ffce71a6e6f"
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    1   14\n",
            "   22   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173\n",
            "   36  256    5   25  100   43  838  112   50  670    2    9   35  480\n",
            "  284    5  150    4  172  112  167    2  336  385   39    4  172 4536\n",
            " 1111   17  546   38   13  447    4  192   50   16    6  147 2025   19\n",
            "   14   22    4 1920 4613  469    4   22   71   87   12   16   43  530\n",
            "   38   76   15   13 1247    4   22   17  515   17   12   16  626   18\n",
            "    2    5   62  386   12    8  316    8  106    5    4 2223 5244   16\n",
            "  480   66 3785   33    4  130   12   16   38  619    5   25  124   51\n",
            "   36  135   48   25 1415   33    6   22   12  215   28   77   52    5\n",
            "   14  407   16   82    2    8    4  107  117 5952   15  256    4    2\n",
            "    7 3766    5  723   36   71   43  530  476   26  400  317   46    7\n",
            "    4    2 1029   13  104   88    4  381   15  297   98   32 2071   56\n",
            "   26  141    6  194 7486   18    4  226   22   21  134  476   26  480\n",
            "    5  144   30 5535   18   51   36   28  224   92   25  104    4  226\n",
            "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
            "   15   16 5345   19  178   32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjlk4DUZWqb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "db31be87-40a5-44c8-dc4a-29a1304ac0dc"
      },
      "source": [
        "word_id = imdb.get_word_index()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfrLEAjjWqb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INDEX_FROM = 3\n",
        "\n",
        "word_id = {k:(v+INDEX_FROM) for k,v in word_id.items()} # Shift all the items to make space for the SPECIAL WORDS\n",
        "word_id[\"_UNK_\"] = 0\n",
        "word_id[\"_START_\"] = 1\n",
        "word_id[\"_CUT_\"] = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbjbRKgZYtVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_word = {v:k for k,v in word_id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrVwHNcXYw2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e6793d8-2d0f-4948-f05f-f52b62ac91dd"
      },
      "source": [
        "id_word[4]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3DGuR_YY0z4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6e41d5c3-d90e-405d-de77-0257fc8a5bb4"
      },
      "source": [
        "print(' '.join(id_word[i] for i in x_train[0] ))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _UNK_ _START_ this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert _CUT_ is an amazing actor and now the same being director _CUT_ father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for _CUT_ and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also _CUT_ to the two little boy's that played the _CUT_ of norman and paul they were just brilliant children are often left out of the _CUT_ list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QzEbCkkWqcB",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9vtUuSEWqcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "TEST_INPUT_INDEX = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2mSfQ4LWqcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_layer_outputs(model, test_input_index=TEST_INPUT_INDEX):\n",
        "    input_ = model.input                                        \n",
        "    outputs = [layer.output for layer in lstm_model.layers]     \n",
        "    func = K.function([input_, K.learning_phase()], outputs )  \n",
        "\n",
        "    # Testing\n",
        "    test = [x_test[test_input_index]]\n",
        "    layer_outs = func([test, 1.])\n",
        "\n",
        "    for i, layer_out in enumerate(layer_outs):\n",
        "        print(\"OUTPUT SHAPE for Layer {} ({}) : {}\".format(i+1, outputs[i].name, layer_out.shape))\n",
        "        print(layer_out)\n",
        "        print()\n",
        "\n",
        "    print(\"EXPECTED OUTPUT LABEL : {}\".format(y_test[test_input_index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAi2LYhFWqcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0137633d-33a3-43b3-ef29-fa5b03b52802"
      },
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(vocab_size, 128))\n",
        "lstm_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0825 15:28:49.384851 140018370099072 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0825 15:28:49.425821 140018370099072 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0825 15:28:49.432460 140018370099072 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0825 15:28:49.560270 140018370099072 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0825 15:28:49.569722 140018370099072 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhcBRlw7ZN_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
        "                                         patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k95uupZiZRcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "ca8189f3-92d5-4130-f48e-0698c79cc3e7"
      },
      "source": [
        "#Compile\n",
        "lstm_model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0825 15:29:19.242911 140018370099072 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0825 15:29:19.398277 140018370099072 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0825 15:29:19.405412 140018370099072 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6eqnRxpZVSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "0565a465-cf3c-4b12-99b1-582fce372190"
      },
      "source": [
        "lstm_model.fit(x_train, y_train, batch_size=64, epochs=20,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[early_stopping])\n",
        "\n",
        "loss, acc = lstm_model.evaluate(x_test, y_test, batch_size=64)\n",
        "\n",
        "print('Test loss (LOWER is better)      :', loss)\n",
        "print('Test accuracy (HIGHER is better) :', acc)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 199s 8ms/step - loss: 0.4873 - acc: 0.7625 - val_loss: 0.4973 - val_acc: 0.7564\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 193s 8ms/step - loss: 0.3662 - acc: 0.8498 - val_loss: 0.4010 - val_acc: 0.8318\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 191s 8ms/step - loss: 0.2897 - acc: 0.8843 - val_loss: 0.3725 - val_acc: 0.8413\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 191s 8ms/step - loss: 0.2333 - acc: 0.9102 - val_loss: 0.3695 - val_acc: 0.8581\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 190s 8ms/step - loss: 0.2907 - acc: 0.8762 - val_loss: 0.3793 - val_acc: 0.8632\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 190s 8ms/step - loss: 0.1842 - acc: 0.9294 - val_loss: 0.3396 - val_acc: 0.8661\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 191s 8ms/step - loss: 0.1391 - acc: 0.9485 - val_loss: 0.3801 - val_acc: 0.8695\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 196s 8ms/step - loss: 0.1095 - acc: 0.9592 - val_loss: 0.4136 - val_acc: 0.8680\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 194s 8ms/step - loss: 0.0790 - acc: 0.9718 - val_loss: 0.4640 - val_acc: 0.8561\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 200s 8ms/step - loss: 0.0648 - acc: 0.9782 - val_loss: 0.5319 - val_acc: 0.8568\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 196s 8ms/step - loss: 0.0516 - acc: 0.9826 - val_loss: 0.5464 - val_acc: 0.8574\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 193s 8ms/step - loss: 0.0443 - acc: 0.9853 - val_loss: 0.5542 - val_acc: 0.8574\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 195s 8ms/step - loss: 0.0350 - acc: 0.9893 - val_loss: 0.7249 - val_acc: 0.8464\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 198s 8ms/step - loss: 0.0304 - acc: 0.9908 - val_loss: 0.6221 - val_acc: 0.8565\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 198s 8ms/step - loss: 0.0246 - acc: 0.9922 - val_loss: 0.6745 - val_acc: 0.8592\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 194s 8ms/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.7491 - val_acc: 0.8554\n",
            "Epoch 00016: early stopping\n",
            "25000/25000 [==============================] - 32s 1ms/step\n",
            "('Test loss (LOWER is better)      :', 0.7491366099262238)\n",
            "('Test accuracy (HIGHER is better) :', 0.8554000000190735)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTac9e--ZbOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "20bbef52-b015-4cbd-d390-cafc87c329f9"
      },
      "source": [
        "print_layer_outputs(lstm_model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT SHAPE for Layer 1 (embedding_1/embedding_lookup/Identity:0) : (1, 300, 128)\n",
            "[[[ 0.16638324  0.04940548  0.03265361 ...  0.13320364  0.02578399\n",
            "   -0.08691566]\n",
            "  [ 0.16638324  0.04940548  0.03265361 ...  0.13320364  0.02578399\n",
            "   -0.08691566]\n",
            "  [ 0.16638324  0.04940548  0.03265361 ...  0.13320364  0.02578399\n",
            "   -0.08691566]\n",
            "  ...\n",
            "  [-0.04995142 -0.04405298  0.06257865 ...  0.06440125 -0.020206\n",
            "   -0.03984457]\n",
            "  [ 0.07828303  0.03545779  0.01321205 ...  0.02114357  0.00130465\n",
            "   -0.05563096]\n",
            "  [-0.01449515  0.05962845 -0.03083712 ...  0.04419639  0.05406221\n",
            "    0.06366812]]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 2 (lstm_1/TensorArrayReadV3:0) : (1, 128)\n",
            "[[ 0.37320197 -0.18202047  0.07033734 -0.32115197 -0.08030289 -0.12198625\n",
            "  -0.00545509  0.21287002 -0.04580798  0.08436047 -0.10227307  0.01241867\n",
            "  -0.22514728 -0.05820316 -0.00446917 -0.10885498  0.11282193  0.09502894\n",
            "   0.00080859 -0.070156   -0.17870657 -0.00718841 -0.19159992  0.04050032\n",
            "  -0.5057934  -0.01663282 -0.19426614 -0.20117939  0.01626331  0.0776046\n",
            "   0.25937462 -0.11807777 -0.00762802  0.03345575  0.11117028 -0.16329843\n",
            "   0.3700758   0.09308197  0.00370961  0.01073514 -0.00763025 -0.0307884\n",
            "   0.1433499  -0.01285803 -0.10417255  0.00516719  0.06393529  0.15325938\n",
            "   0.12305848  0.06056064  0.3628611   0.05593103  0.02029155  0.43148997\n",
            "  -0.15411068 -0.11714606  0.07101293  0.14971878  0.2787189  -0.02268828\n",
            "   0.280376    0.02298642  0.12429658  0.00550684 -0.09671282  0.2673591\n",
            "  -0.05148544 -0.01200231 -0.16272922 -0.05841479  0.26850334  0.00133954\n",
            "  -0.00865508  0.18178625  0.10016987  0.11654036 -0.02374745  0.08369825\n",
            "  -0.26854208 -0.5275135   0.06507576  0.05874471 -0.01961022  0.161209\n",
            "   0.2326781   0.03438126 -0.00830271  0.01315754 -0.0042937  -0.23239172\n",
            "   0.00166601  0.17821781 -0.05671817 -0.22500718 -0.04210694  0.00838613\n",
            "   0.2830997  -0.03924637 -0.02940934 -0.10188264 -0.20053995  0.09935035\n",
            "  -0.04698332 -0.01371512  0.25827104 -0.0989422   0.09804326 -0.08466077\n",
            "   0.02588003  0.22565487  0.16346948 -0.06185165  0.09134413 -0.03997559\n",
            "  -0.0804422   0.0578448  -0.0551652  -0.12073604  0.03795573  0.029176\n",
            "   0.08926322  0.23771796 -0.034635   -0.17600533  0.29127273 -0.4252124\n",
            "  -0.06739075  0.15240064]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 3 (dense_1/Sigmoid:0) : (1, 1)\n",
            "[[0.18295838]]\n",
            "()\n",
            "EXPECTED OUTPUT LABEL : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_k0lWAolwBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v_model = Sequential()\n",
        "v_model.add(Embedding(vocab_size, 128, input_length=maxlen))\n",
        "v_model.add(Flatten())\n",
        "v_model.add(Dense(250, activation='relu'))\n",
        "v_model.add(Dense(1, activation='sigmoid'))\n",
        "v_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_RO5qn3l8kG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "1072a90c-e0ab-43fe-f464-f0fdf75fc13f"
      },
      "source": [
        "#vanilla model\n",
        "v_model.fit(x_train, y_train, batch_size=64, epochs=20,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[early_stopping])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 6s 227us/step - loss: 0.3964 - acc: 0.8076 - val_loss: 0.3035 - val_acc: 0.8699\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 5s 199us/step - loss: 0.0636 - acc: 0.9806 - val_loss: 0.4350 - val_acc: 0.8504\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 5s 198us/step - loss: 0.0055 - acc: 0.9986 - val_loss: 0.5461 - val_acc: 0.8594\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 5s 197us/step - loss: 4.2067e-04 - acc: 1.0000 - val_loss: 0.5886 - val_acc: 0.8607\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 5s 198us/step - loss: 1.2092e-04 - acc: 1.0000 - val_loss: 0.6109 - val_acc: 0.8612\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 5s 197us/step - loss: 7.1868e-05 - acc: 1.0000 - val_loss: 0.6294 - val_acc: 0.8610\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 5s 197us/step - loss: 4.7982e-05 - acc: 1.0000 - val_loss: 0.6450 - val_acc: 0.8620\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 5s 198us/step - loss: 3.3937e-05 - acc: 1.0000 - val_loss: 0.6590 - val_acc: 0.8621\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 5s 199us/step - loss: 2.4690e-05 - acc: 1.0000 - val_loss: 0.6718 - val_acc: 0.8625\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 5s 197us/step - loss: 1.8485e-05 - acc: 1.0000 - val_loss: 0.6839 - val_acc: 0.8630\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 5s 198us/step - loss: 1.4029e-05 - acc: 1.0000 - val_loss: 0.6951 - val_acc: 0.8631\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5847fe64d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1y05qaUmCSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4acdc452-e674-44f6-b92b-0714c0a16d11"
      },
      "source": [
        "loss, acc = v_model.evaluate(x_test, y_test, batch_size=64)\n",
        "\n",
        "print('Test loss (LOWER is better)      :', loss)\n",
        "print('Test accuracy (HIGHER is better) :', acc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 30us/step\n",
            "('Test loss (LOWER is better)      :', 0.6950853090476989)\n",
            "('Test accuracy (HIGHER is better) :', 0.863080000038147)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyvGoI-KmXBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ = v_model.input\n",
        "outputs = [layer.output for layer in v_model.layers]\n",
        "func = K.function([input_, K.learning_phase()], outputs )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyxkZiDOmkJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = [x_test[TEST_INPUT_INDEX]]\n",
        "layer_outs = func([test, 1.])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnq-AM__mth0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37189c58-4449-420c-f12e-a1f8c8d6c7f2"
      },
      "source": [
        "for i, layer_out in enumerate(layer_outs):\n",
        "    print(\"OUTPUT SHAPE for Layer {} ({}) : {}\".format(i+1, outputs[i].name, layer_out.shape))\n",
        "    print(layer_out)\n",
        "    print()\n",
        "\n",
        "print(\"EXPECTED OUTPUT LABEL : {}\".format(y_test[TEST_INPUT_INDEX]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT SHAPE for Layer 1 (embedding_2/embedding_lookup/Identity:0) : (1, 300, 128)\n",
            "[[[-0.0043788  -0.00404044  0.01119822 ...  0.00757719  0.0016334\n",
            "   -0.0039446 ]\n",
            "  [-0.0043788  -0.00404044  0.01119822 ...  0.00757719  0.0016334\n",
            "   -0.0039446 ]\n",
            "  [-0.0043788  -0.00404044  0.01119822 ...  0.00757719  0.0016334\n",
            "   -0.0039446 ]\n",
            "  ...\n",
            "  [-0.00062622  0.0127108   0.02629201 ...  0.03687827  0.04996743\n",
            "   -0.00037604]\n",
            "  [-0.02858324 -0.02247133  0.01345623 ...  0.0132363   0.01304052\n",
            "    0.00252912]\n",
            "  [-0.05393251  0.02738246  0.01452963 ... -0.06496478 -0.12210263\n",
            "   -0.06148017]]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 2 (flatten_1/Reshape:0) : (1, 38400)\n",
            "[[-0.0043788  -0.00404044  0.01119822 ... -0.06496478 -0.12210263\n",
            "  -0.06148017]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 3 (dense_2/Relu:0) : (1, 250)\n",
            "[[0.67135656 0.5776657  0.511561   0.719964   0.5483909  0.47536692\n",
            "  0.64605933 0.4882938  0.6224323  0.56616205 0.42435616 0.36288357\n",
            "  0.63522816 0.0799265  0.5311148  0.         0.6376988  0.5062854\n",
            "  0.57696515 0.5600595  0.49422452 0.45880285 0.75270677 0.57207966\n",
            "  0.60325485 0.81773233 0.         0.         0.26680878 0.6019351\n",
            "  0.5951492  0.62873054 0.50028056 0.76936305 0.7527719  0.\n",
            "  0.720538   0.49008912 0.587776   0.565211   0.5786248  0.\n",
            "  0.40819573 0.         0.         0.4049352  0.6436302  0.4967137\n",
            "  0.         0.23523682 0.4632902  0.         0.6206687  0.6097831\n",
            "  0.4062388  0.6263103  0.64677393 0.709816   0.22054282 0.46301568\n",
            "  0.37205753 0.         0.0566319  0.3417811  0.6083488  0.34493178\n",
            "  0.30986887 0.43425116 0.6643864  0.57893384 0.3892729  0.\n",
            "  0.5860243  0.5060158  0.         0.5293775  0.47303274 0.7114212\n",
            "  0.         0.07635132 0.49179444 0.5563254  0.6975832  0.8845098\n",
            "  0.         0.649271   0.         0.6691511  0.56789845 0.7200229\n",
            "  0.         0.         0.46807757 0.         0.57331765 0.5243376\n",
            "  0.79248357 0.36414635 0.12919188 0.5915633  0.5775856  0.\n",
            "  0.3873399  0.8546503  0.6185328  0.         0.5802495  0.53879136\n",
            "  0.50587887 0.8637886  0.42130312 0.6089988  0.         0.61798203\n",
            "  0.47683302 0.75929147 0.54016006 0.09562828 0.56169796 0.5115813\n",
            "  0.75545496 0.         0.6441861  0.09744503 0.7585298  0.72452\n",
            "  0.6191244  0.         0.         0.61321574 0.24752346 0.5587441\n",
            "  0.6065844  0.45467865 0.         0.6239559  0.         0.10176893\n",
            "  0.71477205 0.23417187 0.08157122 0.54040587 0.         0.8356314\n",
            "  0.88614357 0.7080623  0.6129475  0.48105568 0.5205381  0.06360003\n",
            "  0.         0.6141504  0.5062087  0.266002   0.         0.59923464\n",
            "  0.         0.5499039  0.34421661 0.         0.21588072 0.10392377\n",
            "  0.5386254  0.574743   0.         0.322795   0.62607425 0.\n",
            "  0.56809384 0.12183077 0.31294128 0.5930876  0.         0.55715936\n",
            "  0.         0.         0.         0.446598   0.14273196 0.8150459\n",
            "  0.6921749  0.7078825  0.40505555 0.         0.5659038  0.6106787\n",
            "  0.62238324 0.         0.3998681  0.4912471  0.5648263  0.\n",
            "  0.7177158  0.7021558  0.5660325  0.         0.37697163 0.\n",
            "  0.         0.63132334 0.         0.6195903  0.43769914 0.5399444\n",
            "  0.         0.36813304 0.         0.5486228  0.47275555 0.\n",
            "  0.55227584 0.6058499  0.48346412 0.45527387 0.18641682 0.29080385\n",
            "  0.6563978  0.4671784  0.         0.         0.53448135 0.6419458\n",
            "  0.4569913  0.68708444 0.45551938 0.52139425 0.17075649 0.6349604\n",
            "  0.6055096  0.55213004 0.         0.51498336 0.6333206  0.5083451\n",
            "  0.65710634 0.55360335 0.8505123  0.01324034 0.3976303  0.5481579\n",
            "  0.50792074 0.6339783  0.59103143 0.43694186 0.12554573 0.31735712\n",
            "  0.         0.         0.5538434  0.7364819 ]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 4 (dense_3/Sigmoid:0) : (1, 1)\n",
            "[[0.7794373]]\n",
            "()\n",
            "EXPECTED OUTPUT LABEL : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbC0NtlImw1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f9fb994a-b0e2-4040-ee14-73e1ef5875fb"
      },
      "source": [
        "#Cnn Model\n",
        "\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(vocab_size, 128, input_length=maxlen))\n",
        "cnn_model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(250, activation='relu'))\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0825 16:28:44.541110 140018370099072 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xdWicUnm7tH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "a17ab304-6acd-41db-dcf0-e5cc0714487e"
      },
      "source": [
        "cnn_model.fit(x_train, y_train, batch_size=64, epochs=20,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[early_stopping])\n",
        "\n",
        "loss, acc = cnn_model.evaluate(x_test, y_test, batch_size=64)\n",
        "\n",
        "print('Test loss (LOWER is better)      :', loss)\n",
        "print('Test accuracy (HIGHER is better) :', acc)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 7s 285us/step - loss: 0.3697 - acc: 0.8170 - val_loss: 0.2623 - val_acc: 0.8899\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 4s 144us/step - loss: 0.1513 - acc: 0.9448 - val_loss: 0.2993 - val_acc: 0.8805\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 4s 144us/step - loss: 0.0442 - acc: 0.9877 - val_loss: 0.4794 - val_acc: 0.8644\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 4s 143us/step - loss: 0.0065 - acc: 0.9986 - val_loss: 0.5587 - val_acc: 0.8758\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 4s 147us/step - loss: 7.9238e-04 - acc: 1.0000 - val_loss: 0.6355 - val_acc: 0.8793\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 4s 147us/step - loss: 1.2788e-04 - acc: 1.0000 - val_loss: 0.6572 - val_acc: 0.8801\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 4s 147us/step - loss: 6.8028e-05 - acc: 1.0000 - val_loss: 0.6843 - val_acc: 0.8801\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 4s 146us/step - loss: 4.3989e-05 - acc: 1.0000 - val_loss: 0.7071 - val_acc: 0.8799\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 4s 145us/step - loss: 3.0006e-05 - acc: 1.0000 - val_loss: 0.7265 - val_acc: 0.8807\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 4s 142us/step - loss: 2.1107e-05 - acc: 1.0000 - val_loss: 0.7470 - val_acc: 0.8798\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 4s 142us/step - loss: 1.5033e-05 - acc: 1.0000 - val_loss: 0.7668 - val_acc: 0.8800\n",
            "Epoch 00011: early stopping\n",
            "25000/25000 [==============================] - 1s 29us/step\n",
            "('Test loss (LOWER is better)      :', 0.7667903868770599)\n",
            "('Test accuracy (HIGHER is better) :', 0.8800399999809265)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JRhBABnRMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ = cnn_model.input                               \n",
        "outputs = [layer.output for layer in cnn_model.layers]\n",
        "func = K.function([input_, K.learning_phase()], outputs )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-vNw7hmnl33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = [x_test[TEST_INPUT_INDEX]]\n",
        "layer_outs = func([test, 1.])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIZ0y8OKnpFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed436225-2732-40c9-8748-aed602d8f854"
      },
      "source": [
        "for i, layer_out in enumerate(layer_outs):\n",
        "    print(\"OUTPUT SHAPE for Layer {} ({}) : {}\".format(i+1, outputs[i].name, layer_out.shape))\n",
        "    print(layer_out)\n",
        "    print()\n",
        "\n",
        "print(\"EXPECTED OUTPUT LABEL : {}\".format(y_test[TEST_INPUT_INDEX]))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT SHAPE for Layer 1 (embedding_3/embedding_lookup/Identity:0) : (1, 300, 128)\n",
            "[[[-0.03964212 -0.0802965  -0.05474008 ...  0.01203048 -0.04466333\n",
            "    0.0506419 ]\n",
            "  [-0.03964212 -0.0802965  -0.05474008 ...  0.01203048 -0.04466333\n",
            "    0.0506419 ]\n",
            "  [-0.03964212 -0.0802965  -0.05474008 ...  0.01203048 -0.04466333\n",
            "    0.0506419 ]\n",
            "  ...\n",
            "  [ 0.00441057  0.04100005  0.01868439 ...  0.04971323  0.04151232\n",
            "   -0.02462053]\n",
            "  [-0.11718191  0.00831428 -0.02294085 ... -0.05033892  0.01161678\n",
            "    0.02111319]\n",
            "  [ 0.03288076  0.04441469 -0.00708851 ... -0.03903577  0.03864863\n",
            "    0.02397821]]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 2 (conv1d_1/Relu:0) : (1, 300, 64)\n",
            "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.         0.1331963  0.         ... 0.         0.00190557 0.        ]\n",
            "  [0.         0.         0.         ... 0.06535861 0.         0.        ]\n",
            "  [0.         0.         0.11581001 ... 0.         0.         0.        ]]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 3 (max_pooling1d_1/Squeeze:0) : (1, 150, 64)\n",
            "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.         0.         0.10603517 ... 0.05970782 0.         0.        ]\n",
            "  [0.         0.1331963  0.         ... 0.         0.00190557 0.        ]\n",
            "  [0.         0.         0.11581001 ... 0.06535861 0.         0.        ]]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 4 (flatten_2/Reshape:0) : (1, 9600)\n",
            "[[0.         0.         0.         ... 0.06535861 0.         0.        ]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 5 (dense_4/Relu:0) : (1, 250)\n",
            "[[0.35261232 0.         1.076583   1.5330583  0.42365733 0.\n",
            "  1.4894052  0.         0.         0.         0.         0.6281605\n",
            "  0.         1.0923067  0.01588766 0.7429877  0.6467111  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.4545422  0.         0.         0.62268716\n",
            "  0.         0.         0.         0.         1.4456795  0.25012642\n",
            "  0.7254546  1.4997442  1.5990878  0.         1.477639   1.5044882\n",
            "  0.         0.         0.4196861  0.61120105 0.         0.\n",
            "  0.         0.5347779  1.4960233  1.532254   0.         0.7853569\n",
            "  1.5771825  0.         1.3110608  0.         0.         0.\n",
            "  0.         1.0768728  0.         0.         0.8779237  0.\n",
            "  1.4561672  0.26647428 1.5729363  1.5196481  0.         0.\n",
            "  0.         0.54078734 0.         0.7673583  1.7810826  2.7334547\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.1808529  0.\n",
            "  0.60064906 0.         0.61718017 1.5625621  0.64197546 0.61150694\n",
            "  0.         1.2846692  0.         0.         0.28855124 0.\n",
            "  0.         0.         0.5387869  0.         0.46379587 0.743307\n",
            "  0.         0.         0.         0.60269034 0.         0.\n",
            "  0.         1.781448   0.         0.34386316 0.         1.4930301\n",
            "  0.         0.         0.         0.7276541  0.         0.\n",
            "  1.5259978  0.         0.         1.5305707  0.         0.\n",
            "  0.         1.2270741  0.         1.0386748  0.         0.\n",
            "  0.         0.         0.         0.54712474 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.6718803\n",
            "  0.69838446 0.         1.4164113  0.         0.         0.\n",
            "  0.         0.913721   1.5820351  0.75763345 1.0413126  0.\n",
            "  0.         0.         0.67357904 0.         0.         0.7653129\n",
            "  0.         0.         0.6609262  0.         0.         0.\n",
            "  0.84392923 0.         2.2989552  0.6037217  0.         0.4906907\n",
            "  1.3245412  0.72393906 0.         0.         0.         0.\n",
            "  0.5499685  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.35570928 1.3715165  1.5660809  0.\n",
            "  0.         0.7721153  0.         1.0694358  0.         2.4003863\n",
            "  0.         1.3558317  1.5694954  0.         1.5118245  0.\n",
            "  0.         0.         1.3327603  0.         0.         0.6791615\n",
            "  0.         0.         0.         0.         1.5132042  0.\n",
            "  0.         0.         0.         0.51244235 0.96168554 1.60445\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.0261029  0.         0.         0.         0.\n",
            "  0.         1.4519117  0.51638496 0.         0.         0.\n",
            "  0.         0.         1.942598   0.        ]]\n",
            "()\n",
            "OUTPUT SHAPE for Layer 6 (dense_5/Sigmoid:0) : (1, 1)\n",
            "[[0.01945474]]\n",
            "()\n",
            "EXPECTED OUTPUT LABEL : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZsxfwPinryf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Logistic Regression\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss, f1_score\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0iETKRtnzrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "d31e5800-1857-40ba-b93b-80f4f667e5ab"
      },
      "source": [
        "x_train_s = [' '.join(map(str, row)) for row in x_train]\n",
        "x_test_s = [' '.join(map(str, row)) for row in x_test]\n",
        "\n",
        "# We'll just use the default values.\n",
        "pipeline = Pipeline([('counter', TfidfVectorizer()), \n",
        "                     ('classifier', LogisticRegression())])\n",
        "pipeline.fit(x_train_s, y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('counter', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
              "        dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf...penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc14TdBfn42X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e8358492-4a34-4f90-b3ea-0697ad134c73"
      },
      "source": [
        "y_sklearn = pipeline.predict(x_test_s)\n",
        "y_proba_sklearn = pipeline.predict_proba(x_test_s)\n",
        "\n",
        "print('Test loss     : {}'.format(log_loss(y_test, y_proba_sklearn)))\n",
        "print('Test accuracy : {}'.format(accuracy_score(y_test, y_sklearn)))\n",
        "print('Test f1 score : {}'.format(f1_score(y_test, y_sklearn)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss     : 0.310943791863\n",
            "Test accuracy : 0.88408\n",
            "Test f1 score : 0.884357541899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04oU4F7qoDWZ",
        "colab_type": "text"
      },
      "source": [
        "**Sentiment Analysis Observation**\n",
        "\n",
        "**LSTM**\n",
        "\n",
        "* Test loss (LOWER is better) : 0.6593842343902588\n",
        "* Test accuracy (HIGHER is better) : 0.8450000000190735\n",
        "* Epochs : 13\n",
        "* Overfitted : Yes\n",
        "\n",
        "**NN/MLP**\n",
        "\n",
        "* Test loss (LOWER is better) : 0.7345848669528962\n",
        "* Test accuracy (HIGHER is better) : 0.8631200000381469\n",
        "* Epochs : 11\n",
        "* Overfitted : Yes\n",
        "\n",
        "**CNN**\n",
        "\n",
        "* Test loss (LOWER is better) : 0.7345374908304214\n",
        "* Test accuracy (HIGHER is better) : 0.8804400000190735\n",
        "* Epochs : 11\n",
        "* Overfitted : Yes\n",
        "\n",
        "**Logistic Regression**\n",
        "\n",
        "Test loss : 0.31094379459836513\n",
        "Test accuracy : 0.88408\n",
        "Test f1 score : 0.8843575418994414\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The LSTM based RNN was the slowest to overfit. \n",
        "\n",
        "The MLP and the CNN quickly overfitted. \n",
        "\n",
        "Even though the LSTM based RNN has the least accuracy score, it is likely to generalize better if the dataset is bigger because of it ability to learn complex relations over larger contexts. \n",
        "\n",
        "As such with the smaller dataset provided to these models, a simple Logistic Regression performed better!"
      ]
    }
  ]
}